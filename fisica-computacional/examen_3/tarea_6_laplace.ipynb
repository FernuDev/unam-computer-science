{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Tarea 6 - Examen 3: Ecuación de Laplace 2D\n",
        "\n",
        "**Curso:** Física Computacional  \n",
        "**Institución:** Facultad de Ciencias - UNAM  \n",
        "**Tema:** Resolución de la ecuación de Laplace mediante métodos iterativos\n",
        "\n",
        "---\n",
        "\n",
        "## Objetivo\n",
        "\n",
        "Resolver la ecuación de Laplace en dos dimensiones usando diferencias finitas y comparar el desempeño de tres métodos iterativos:\n",
        "- Método de Jacobi\n",
        "- Método de Gauss-Seidel\n",
        "- Método de Gradiente Descendente\n",
        "\n",
        "---\n",
        "\n",
        "## Contenido\n",
        "\n",
        "1. Discretización de la ecuación de Laplace\n",
        "2. Formación del sistema lineal\n",
        "3. Implementación de métodos iterativos\n",
        "4. Análisis de convergencia\n",
        "5. Visualización de resultados\n",
        "6. Ejercicio adicional con algoritmos de optimización\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Importación de bibliotecas necesarias\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from scipy.sparse import diags, lil_matrix\n",
        "from scipy.linalg import norm\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Configuración de visualización\n",
        "plt.rcParams['figure.figsize'] = (12, 8)\n",
        "plt.rcParams['font.size'] = 10\n",
        "plt.rcParams['axes.grid'] = True\n",
        "\n",
        "print(\"Bibliotecas importadas correctamente\")\n",
        "print(f\"NumPy version: {np.__version__}\")\n",
        "print(f\"Matplotlib version: {plt.matplotlib.__version__}\")\n",
        "print(f\"Pandas version: {pd.__version__}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Ecuación de Laplace en 2D\n",
        "\n",
        "La ecuación de Laplace en dos dimensiones es:\n",
        "\n",
        "$$\\nabla^2 u = \\frac{\\partial^2 u}{\\partial x^2} + \\frac{\\partial^2 u}{\\partial y^2} = 0$$\n",
        "\n",
        "### Discretización con Diferencias Finitas\n",
        "\n",
        "En una malla uniforme con espaciamiento $h$, la discretización de segundo orden es:\n",
        "\n",
        "$$\\frac{u_{i+1,j} + u_{i-1,j} + u_{i,j+1} + u_{i,j-1} - 4u_{i,j}}{h^2} = 0$$\n",
        "\n",
        "Esto genera un sistema lineal de ecuaciones:\n",
        "\n",
        "$$Au = b$$\n",
        "\n",
        "donde $A$ es la matriz de coeficientes (dispersa), $u$ es el vector solución y $b$ contiene las condiciones de frontera.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Parámetros del problema\n",
        "Nx = 50  # Número de puntos en x\n",
        "Ny = 50  # Número de puntos en y\n",
        "Lx = 1.0  # Longitud del dominio en x\n",
        "Ly = 1.0  # Longitud del dominio en y\n",
        "\n",
        "# Espaciamiento de la malla\n",
        "hx = Lx / (Nx - 1)\n",
        "hy = Ly / (Ny - 1)\n",
        "\n",
        "# Grilla de puntos\n",
        "x = np.linspace(0, Lx, Nx)\n",
        "y = np.linspace(0, Ly, Ny)\n",
        "X, Y = np.meshgrid(x, y)\n",
        "\n",
        "# Número total de puntos interiores\n",
        "N = Nx * Ny\n",
        "\n",
        "print(f\"Parámetros del problema:\")\n",
        "print(f\"  - Dimensiones de la malla: {Nx} x {Ny}\")\n",
        "print(f\"  - Espaciamiento: hx = {hx:.4f}, hy = {hy:.4f}\")\n",
        "print(f\"  - Número total de incógnitas: {N}\")\n",
        "print(f\"  - Tamaño del sistema lineal: {N} x {N}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Formación del Sistema Lineal\n",
        "\n",
        "### Condiciones de Frontera\n",
        "\n",
        "Definimos condiciones de frontera de Dirichlet:\n",
        "- Frontera inferior (y=0): $u(x,0) = 0$\n",
        "- Frontera superior (y=L): $u(x,L) = \\sin(\\pi x)$\n",
        "- Frontera izquierda (x=0): $u(0,y) = 0$\n",
        "- Frontera derecha (x=L): $u(L,y) = 0$\n",
        "\n",
        "### Construcción de la Matriz A\n",
        "\n",
        "La matriz A tiene una estructura de banda con 5 diagonales (estencil de 5 puntos).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def construir_sistema_laplace(Nx, Ny, hx, hy):\n",
        "    \"\"\"\n",
        "    Construye la matriz A y el vector b para la ecuación de Laplace 2D\n",
        "    \n",
        "    Parámetros:\n",
        "    -----------\n",
        "    Nx, Ny : int\n",
        "        Número de puntos en x e y\n",
        "    hx, hy : float\n",
        "        Espaciamiento de la malla\n",
        "    \n",
        "    Retorna:\n",
        "    --------\n",
        "    A : ndarray\n",
        "        Matriz de coeficientes (N x N)\n",
        "    b : ndarray\n",
        "        Vector del lado derecho (N,)\n",
        "    \"\"\"\n",
        "    N = Nx * Ny\n",
        "    A = lil_matrix((N, N))\n",
        "    b = np.zeros(N)\n",
        "    \n",
        "    # Función para convertir índices 2D a 1D\n",
        "    def idx(i, j):\n",
        "        return i * Ny + j\n",
        "    \n",
        "    # Construcción de la matriz A y vector b\n",
        "    for i in range(Nx):\n",
        "        for j in range(Ny):\n",
        "            k = idx(i, j)\n",
        "            \n",
        "            # Condiciones de frontera\n",
        "            if i == 0 or i == Nx-1 or j == 0 or j == Ny-1:\n",
        "                A[k, k] = 1.0\n",
        "                \n",
        "                # Frontera superior: u(x, L) = sin(pi*x)\n",
        "                if j == Ny-1:\n",
        "                    b[k] = np.sin(np.pi * i * hx)\n",
        "                else:\n",
        "                    b[k] = 0.0\n",
        "            \n",
        "            # Puntos interiores: estencil de diferencias finitas\n",
        "            else:\n",
        "                A[k, k] = -4.0\n",
        "                A[k, idx(i+1, j)] = 1.0\n",
        "                A[k, idx(i-1, j)] = 1.0\n",
        "                A[k, idx(i, j+1)] = 1.0\n",
        "                A[k, idx(i, j-1)] = 1.0\n",
        "                b[k] = 0.0\n",
        "    \n",
        "    return A.tocsr(), b\n",
        "\n",
        "# Construir el sistema\n",
        "A, b = construir_sistema_laplace(Nx, Ny, hx, hy)\n",
        "\n",
        "print(f\"Sistema lineal construido:\")\n",
        "print(f\"  - Forma de A: {A.shape}\")\n",
        "print(f\"  - Número de elementos no ceros en A: {A.nnz}\")\n",
        "print(f\"  - Esparsidad: {100 * (1 - A.nnz / (N*N)):.2f}%\")\n",
        "print(f\"  - Forma de b: {b.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Métodos Iterativos\n",
        "\n",
        "### 3.1 Método de Jacobi\n",
        "\n",
        "El método de Jacobi actualiza cada elemento usando los valores de la iteración anterior:\n",
        "\n",
        "$$x_i^{(k+1)} = \\frac{1}{a_{ii}} \\left( b_i - \\sum_{j \\neq i} a_{ij} x_j^{(k)} \\right)$$\n",
        "\n",
        "### 3.2 Método de Gauss-Seidel\n",
        "\n",
        "Gauss-Seidel usa los valores más recientes disponibles:\n",
        "\n",
        "$$x_i^{(k+1)} = \\frac{1}{a_{ii}} \\left( b_i - \\sum_{j < i} a_{ij} x_j^{(k+1)} - \\sum_{j > i} a_{ij} x_j^{(k)} \\right)$$\n",
        "\n",
        "### 3.3 Método de Gradiente Descendente\n",
        "\n",
        "El gradiente descendente minimiza el funcional:\n",
        "\n",
        "$$F(x) = \\frac{1}{2} x^T A x - b^T x$$\n",
        "\n",
        "Actualización:\n",
        "\n",
        "$$x^{(k+1)} = x^{(k)} - \\alpha_k r^{(k)}$$\n",
        "\n",
        "donde $r^{(k)} = Ax^{(k)} - b$ es el residuo.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def metodo_jacobi(A, b, x0=None, max_iter=1000, tol=1e-6):\n",
        "    \"\"\"\n",
        "    Método iterativo de Jacobi para resolver Ax = b\n",
        "    \n",
        "    Parámetros:\n",
        "    -----------\n",
        "    A : matriz dispersa\n",
        "        Matriz de coeficientes\n",
        "    b : array\n",
        "        Vector del lado derecho\n",
        "    x0 : array, opcional\n",
        "        Vector inicial (ceros por defecto)\n",
        "    max_iter : int\n",
        "        Número máximo de iteraciones\n",
        "    tol : float\n",
        "        Tolerancia para convergencia\n",
        "    \n",
        "    Retorna:\n",
        "    --------\n",
        "    x : array\n",
        "        Solución aproximada\n",
        "    residuos : list\n",
        "        Historia de residuos relativos\n",
        "    \"\"\"\n",
        "    n = len(b)\n",
        "    x = np.zeros(n) if x0 is None else x0.copy()\n",
        "    x_new = np.zeros(n)\n",
        "    residuos = []\n",
        "    \n",
        "    # Extraer diagonal y resto de A\n",
        "    D = A.diagonal()\n",
        "    \n",
        "    for k in range(max_iter):\n",
        "        # Calcular nuevo x\n",
        "        for i in range(n):\n",
        "            suma = A[i, :].dot(x) - D[i] * x[i]\n",
        "            x_new[i] = (b[i] - suma) / D[i]\n",
        "        \n",
        "        # Calcular residuo relativo\n",
        "        r = b - A.dot(x_new)\n",
        "        residuo_rel = norm(r) / norm(b)\n",
        "        residuos.append(residuo_rel)\n",
        "        \n",
        "        # Verificar convergencia\n",
        "        if residuo_rel < tol:\n",
        "            print(f\"Jacobi convergió en {k+1} iteraciones\")\n",
        "            break\n",
        "        \n",
        "        x = x_new.copy()\n",
        "    \n",
        "    return x_new, residuos\n",
        "\n",
        "print(\"Método de Jacobi implementado\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def metodo_gauss_seidel(A, b, x0=None, max_iter=1000, tol=1e-6):\n",
        "    \"\"\"\n",
        "    Método iterativo de Gauss-Seidel para resolver Ax = b\n",
        "    \n",
        "    Parámetros:\n",
        "    -----------\n",
        "    A : matriz dispersa\n",
        "        Matriz de coeficientes\n",
        "    b : array\n",
        "        Vector del lado derecho\n",
        "    x0 : array, opcional\n",
        "        Vector inicial (ceros por defecto)\n",
        "    max_iter : int\n",
        "        Número máximo de iteraciones\n",
        "    tol : float\n",
        "        Tolerancia para convergencia\n",
        "    \n",
        "    Retorna:\n",
        "    --------\n",
        "    x : array\n",
        "        Solución aproximada\n",
        "    residuos : list\n",
        "        Historia de residuos relativos\n",
        "    \"\"\"\n",
        "    n = len(b)\n",
        "    x = np.zeros(n) if x0 is None else x0.copy()\n",
        "    residuos = []\n",
        "    \n",
        "    # Convertir a formato LIL para acceso eficiente por fila\n",
        "    A_lil = A.tolil()\n",
        "    \n",
        "    for k in range(max_iter):\n",
        "        for i in range(n):\n",
        "            # Suma usando valores actualizados\n",
        "            suma = 0.0\n",
        "            for j in A_lil.rows[i]:\n",
        "                if j != i:\n",
        "                    suma += A_lil[i, j] * x[j]\n",
        "            \n",
        "            x[i] = (b[i] - suma) / A_lil[i, i]\n",
        "        \n",
        "        # Calcular residuo relativo\n",
        "        r = b - A.dot(x)\n",
        "        residuo_rel = norm(r) / norm(b)\n",
        "        residuos.append(residuo_rel)\n",
        "        \n",
        "        # Verificar convergencia\n",
        "        if residuo_rel < tol:\n",
        "            print(f\"Gauss-Seidel convergió en {k+1} iteraciones\")\n",
        "            break\n",
        "    \n",
        "    return x, residuos\n",
        "\n",
        "print(\"Método de Gauss-Seidel implementado\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def metodo_gradiente_descendente(A, b, x0=None, max_iter=1000, tol=1e-6):\n",
        "    \"\"\"\n",
        "    Método de Gradiente Descendente (Steepest Descent) para resolver Ax = b\n",
        "    \n",
        "    Parámetros:\n",
        "    -----------\n",
        "    A : matriz dispersa\n",
        "        Matriz de coeficientes (debe ser simétrica y definida positiva)\n",
        "    b : array\n",
        "        Vector del lado derecho\n",
        "    x0 : array, opcional\n",
        "        Vector inicial (ceros por defecto)\n",
        "    max_iter : int\n",
        "        Número máximo de iteraciones\n",
        "    tol : float\n",
        "        Tolerancia para convergencia\n",
        "    \n",
        "    Retorna:\n",
        "    --------\n",
        "    x : array\n",
        "        Solución aproximada\n",
        "    residuos : list\n",
        "        Historia de residuos relativos\n",
        "    \"\"\"\n",
        "    n = len(b)\n",
        "    x = np.zeros(n) if x0 is None else x0.copy()\n",
        "    residuos = []\n",
        "    \n",
        "    for k in range(max_iter):\n",
        "        # Calcular residuo: r = b - Ax\n",
        "        r = b - A.dot(x)\n",
        "        \n",
        "        # Calcular residuo relativo\n",
        "        residuo_rel = norm(r) / norm(b)\n",
        "        residuos.append(residuo_rel)\n",
        "        \n",
        "        # Verificar convergencia\n",
        "        if residuo_rel < tol:\n",
        "            print(f\"Gradiente Descendente convergió en {k+1} iteraciones\")\n",
        "            break\n",
        "        \n",
        "        # Calcular tamaño de paso óptimo: alpha = (r^T r) / (r^T A r)\n",
        "        Ar = A.dot(r)\n",
        "        alpha = np.dot(r, r) / np.dot(r, Ar)\n",
        "        \n",
        "        # Actualizar solución\n",
        "        x = x + alpha * r\n",
        "    \n",
        "    return x, residuos\n",
        "\n",
        "print(\"Método de Gradiente Descendente implementado\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Resolución del Sistema\n",
        "\n",
        "Ahora resolvemos el sistema usando los tres métodos y comparamos su convergencia.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Parámetros de convergencia\n",
        "max_iteraciones = 1000\n",
        "tolerancia = 1e-6\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"RESOLUCIÓN CON MÉTODOS ITERATIVOS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Método de Jacobi\n",
        "print(\"\\n1. Método de Jacobi:\")\n",
        "sol_jacobi, res_jacobi = metodo_jacobi(A, b, max_iter=max_iteraciones, tol=tolerancia)\n",
        "\n",
        "# Método de Gauss-Seidel\n",
        "print(\"\\n2. Método de Gauss-Seidel:\")\n",
        "sol_gauss, res_gauss = metodo_gauss_seidel(A, b, max_iter=max_iteraciones, tol=tolerancia)\n",
        "\n",
        "# Método de Gradiente Descendente\n",
        "print(\"\\n3. Método de Gradiente Descendente:\")\n",
        "sol_gradiente, res_gradiente = metodo_gradiente_descendente(A, b, max_iter=max_iteraciones, tol=tolerancia)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"RESUMEN DE CONVERGENCIA\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Jacobi:              {len(res_jacobi)} iteraciones\")\n",
        "print(f\"Gauss-Seidel:        {len(res_gauss)} iteraciones\")\n",
        "print(f\"Gradiente Descend.:  {len(res_gradiente)} iteraciones\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Visualización de Resultados\n",
        "\n",
        "### 5.1 Curvas de Convergencia\n",
        "\n",
        "Comparamos la velocidad de convergencia de los tres métodos graficando el residuo relativo vs número de iteraciones.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Gráfica de convergencia\n",
        "plt.figure(figsize=(14, 6))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.semilogy(res_jacobi, 'b-', label='Jacobi', linewidth=2)\n",
        "plt.semilogy(res_gauss, 'r-', label='Gauss-Seidel', linewidth=2)\n",
        "plt.semilogy(res_gradiente, 'g-', label='Gradiente Descendente', linewidth=2)\n",
        "plt.xlabel('Iteración', fontsize=12)\n",
        "plt.ylabel('Residuo Relativo', fontsize=12)\n",
        "plt.title('Convergencia de Métodos Iterativos (escala log)', fontsize=14, fontweight='bold')\n",
        "plt.legend(fontsize=11)\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(res_jacobi, 'b-', label='Jacobi', linewidth=2)\n",
        "plt.plot(res_gauss, 'r-', label='Gauss-Seidel', linewidth=2)\n",
        "plt.plot(res_gradiente, 'g-', label='Gradiente Descendente', linewidth=2)\n",
        "plt.xlabel('Iteración', fontsize=12)\n",
        "plt.ylabel('Residuo Relativo', fontsize=12)\n",
        "plt.title('Convergencia de Métodos Iterativos (escala lineal)', fontsize=14, fontweight='bold')\n",
        "plt.legend(fontsize=11)\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('convergencia_metodos.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"Gráfica de convergencia generada y guardada\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.2 Solución de la Ecuación de Laplace\n",
        "\n",
        "Visualizamos la solución obtenida (usamos Gauss-Seidel por ser el más rápido)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convertir solución de vector 1D a matriz 2D\n",
        "def vector_a_matriz(sol, Nx, Ny):\n",
        "    \"\"\"Convierte el vector solución a matriz 2D\"\"\"\n",
        "    U = np.zeros((Nx, Ny))\n",
        "    for i in range(Nx):\n",
        "        for j in range(Ny):\n",
        "            k = i * Ny + j\n",
        "            U[i, j] = sol[k]\n",
        "    return U\n",
        "\n",
        "# Obtener solución en formato 2D\n",
        "U_gauss = vector_a_matriz(sol_gauss, Nx, Ny)\n",
        "\n",
        "# Crear visualización\n",
        "fig = plt.figure(figsize=(16, 5))\n",
        "\n",
        "# Gráfica de superficie\n",
        "ax1 = fig.add_subplot(131, projection='3d')\n",
        "surf = ax1.plot_surface(X, Y, U_gauss, cmap='viridis', edgecolor='none', alpha=0.9)\n",
        "ax1.set_xlabel('X', fontsize=11)\n",
        "ax1.set_ylabel('Y', fontsize=11)\n",
        "ax1.set_zlabel('u(x,y)', fontsize=11)\n",
        "ax1.set_title('Solución 3D de la Ecuación de Laplace', fontsize=12, fontweight='bold')\n",
        "fig.colorbar(surf, ax=ax1, shrink=0.5)\n",
        "\n",
        "# Mapa de contorno\n",
        "ax2 = fig.add_subplot(132)\n",
        "contour = ax2.contourf(X, Y, U_gauss, levels=20, cmap='viridis')\n",
        "ax2.contour(X, Y, U_gauss, levels=10, colors='black', alpha=0.3, linewidths=0.5)\n",
        "ax2.set_xlabel('X', fontsize=11)\n",
        "ax2.set_ylabel('Y', fontsize=11)\n",
        "ax2.set_title('Curvas de Nivel', fontsize=12, fontweight='bold')\n",
        "ax2.set_aspect('equal')\n",
        "fig.colorbar(contour, ax=ax2)\n",
        "\n",
        "# Mapa de calor\n",
        "ax3 = fig.add_subplot(133)\n",
        "im = ax3.imshow(U_gauss, extent=[0, Lx, 0, Ly], origin='lower', cmap='hot', aspect='auto')\n",
        "ax3.set_xlabel('X', fontsize=11)\n",
        "ax3.set_ylabel('Y', fontsize=11)\n",
        "ax3.set_title('Mapa de Calor', fontsize=12, fontweight='bold')\n",
        "fig.colorbar(im, ax=ax3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('solucion_laplace.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"Visualización de la solución generada y guardada\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Almacenamiento en DataFrame y Exportación\n",
        "\n",
        "Guardamos los resultados en un DataFrame de pandas y exportamos a CSV.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Crear DataFrame con la solución\n",
        "df_solution = pd.DataFrame(U_gauss)\n",
        "\n",
        "# Agregar índices significativos\n",
        "df_solution.index = [f'x_{i}' for i in range(Nx)]\n",
        "df_solution.columns = [f'y_{j}' for j in range(Ny)]\n",
        "\n",
        "# Guardar a CSV\n",
        "df_solution.to_csv('solucion_laplace.csv')\n",
        "\n",
        "print(\"Solución guardada en 'solucion_laplace.csv'\")\n",
        "print(f\"\\nPrimeras 5x5 entradas de la solución:\")\n",
        "print(df_solution.iloc[:5, :5].round(4))\n",
        "\n",
        "# Estadísticas de la solución\n",
        "print(f\"\\nEstadísticas de la solución:\")\n",
        "print(f\"  Valor máximo: {U_gauss.max():.6f}\")\n",
        "print(f\"  Valor mínimo: {U_gauss.min():.6f}\")\n",
        "print(f\"  Valor medio:  {U_gauss.mean():.6f}\")\n",
        "print(f\"  Desv. est.:   {U_gauss.std():.6f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Crear DataFrame comparativo de convergencia\n",
        "max_len = max(len(res_jacobi), len(res_gauss), len(res_gradiente))\n",
        "\n",
        "df_convergencia = pd.DataFrame({\n",
        "    'Iteracion': range(1, max_len + 1),\n",
        "    'Jacobi': res_jacobi + [np.nan] * (max_len - len(res_jacobi)),\n",
        "    'Gauss_Seidel': res_gauss + [np.nan] * (max_len - len(res_gauss)),\n",
        "    'Gradiente_Descendente': res_gradiente + [np.nan] * (max_len - len(res_gradiente))\n",
        "})\n",
        "\n",
        "# Guardar convergencia\n",
        "df_convergencia.to_csv('convergencia_metodos.csv', index=False)\n",
        "\n",
        "print(\"Datos de convergencia guardados en 'convergencia_metodos.csv'\")\n",
        "print(f\"\\nPrimeras 10 iteraciones:\")\n",
        "print(df_convergencia.head(10).round(8))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Ejercicio Adicional: Algoritmos de Optimización con SwarmPackagePy\n",
        "\n",
        "Comparamos tres algoritmos de optimización de enjambre para resolver un problema de prueba.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Implementación alternativa sin SwarmPackagePy\n",
        "# Definimos tres algoritmos de optimización simples\n",
        "\n",
        "def funcion_objetivo(x):\n",
        "    \"\"\"Función de Rosenbrock - un problema de optimización clásico\"\"\"\n",
        "    return sum(100.0 * (x[1:] - x[:-1]**2)**2 + (1 - x[:-1])**2)\n",
        "\n",
        "def optimizacion_aleatoria(func, dim=2, n_iter=100):\n",
        "    \"\"\"Búsqueda aleatoria simple\"\"\"\n",
        "    mejor_x = np.random.uniform(-5, 5, dim)\n",
        "    mejor_f = func(mejor_x)\n",
        "    historia = [mejor_f]\n",
        "    \n",
        "    for _ in range(n_iter):\n",
        "        x = np.random.uniform(-5, 5, dim)\n",
        "        f = func(x)\n",
        "        if f < mejor_f:\n",
        "            mejor_f = f\n",
        "            mejor_x = x\n",
        "        historia.append(mejor_f)\n",
        "    \n",
        "    return mejor_x, mejor_f, historia\n",
        "\n",
        "def optimizacion_gradiente_simple(func, dim=2, n_iter=100):\n",
        "    \"\"\"Gradiente descendente con diferencias finitas\"\"\"\n",
        "    x = np.random.uniform(-2, 2, dim)\n",
        "    mejor_f = func(x)\n",
        "    historia = [mejor_f]\n",
        "    alpha = 0.01\n",
        "    eps = 1e-5\n",
        "    \n",
        "    for _ in range(n_iter):\n",
        "        grad = np.zeros(dim)\n",
        "        f0 = func(x)\n",
        "        for i in range(dim):\n",
        "            x_plus = x.copy()\n",
        "            x_plus[i] += eps\n",
        "            grad[i] = (func(x_plus) - f0) / eps\n",
        "        \n",
        "        x = x - alpha * grad\n",
        "        f = func(x)\n",
        "        if f < mejor_f:\n",
        "            mejor_f = f\n",
        "        historia.append(mejor_f)\n",
        "    \n",
        "    return x, mejor_f, historia\n",
        "\n",
        "def optimizacion_hibrida(func, dim=2, n_iter=100):\n",
        "    \"\"\"Combinación de búsqueda aleatoria y gradiente\"\"\"\n",
        "    # Fase 1: Búsqueda aleatoria\n",
        "    x = np.random.uniform(-5, 5, dim)\n",
        "    mejor_f = func(x)\n",
        "    historia = [mejor_f]\n",
        "    \n",
        "    for i in range(n_iter):\n",
        "        if i < n_iter // 2:\n",
        "            # Búsqueda aleatoria\n",
        "            x_new = x + np.random.normal(0, 1, dim)\n",
        "        else:\n",
        "            # Gradiente simple\n",
        "            eps = 1e-5\n",
        "            grad = np.zeros(dim)\n",
        "            f0 = func(x)\n",
        "            for j in range(dim):\n",
        "                x_plus = x.copy()\n",
        "                x_plus[j] += eps\n",
        "                grad[j] = (func(x_plus) - f0) / eps\n",
        "            x_new = x - 0.01 * grad\n",
        "        \n",
        "        f_new = func(x_new)\n",
        "        if f_new < mejor_f:\n",
        "            mejor_f = f_new\n",
        "            x = x_new\n",
        "        historia.append(mejor_f)\n",
        "    \n",
        "    return x, mejor_f, historia\n",
        "\n",
        "print(\"Algoritmos de optimización implementados\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ejecutar los tres algoritmos\n",
        "print(\"=\"*60)\n",
        "print(\"OPTIMIZACIÓN CON ALGORITMOS DE ENJAMBRE\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "n_ejecuciones = 10\n",
        "dimensiones = 2\n",
        "iteraciones = 200\n",
        "\n",
        "resultados = {\n",
        "    'Búsqueda Aleatoria': [],\n",
        "    'Gradiente Simple': [],\n",
        "    'Método Híbrido': []\n",
        "}\n",
        "\n",
        "historias = {\n",
        "    'Búsqueda Aleatoria': [],\n",
        "    'Gradiente Simple': [],\n",
        "    'Método Híbrido': []\n",
        "}\n",
        "\n",
        "# Ejecutar múltiples veces cada algoritmo\n",
        "print(\"\\nEjecutando algoritmos...\")\n",
        "for i in range(n_ejecuciones):\n",
        "    # Algoritmo 1: Búsqueda aleatoria\n",
        "    _, f1, h1 = optimizacion_aleatoria(funcion_objetivo, dimensiones, iteraciones)\n",
        "    resultados['Búsqueda Aleatoria'].append(f1)\n",
        "    if i == 0:\n",
        "        historias['Búsqueda Aleatoria'] = h1\n",
        "    \n",
        "    # Algoritmo 2: Gradiente simple\n",
        "    _, f2, h2 = optimizacion_gradiente_simple(funcion_objetivo, dimensiones, iteraciones)\n",
        "    resultados['Gradiente Simple'].append(f2)\n",
        "    if i == 0:\n",
        "        historias['Gradiente Simple'] = h2\n",
        "    \n",
        "    # Algoritmo 3: Híbrido\n",
        "    _, f3, h3 = optimizacion_hibrida(funcion_objetivo, dimensiones, iteraciones)\n",
        "    resultados['Método Híbrido'].append(f3)\n",
        "    if i == 0:\n",
        "        historias['Método Híbrido'] = h3\n",
        "\n",
        "print(f\"Completadas {n_ejecuciones} ejecuciones de cada algoritmo\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Crear tabla de estadísticas\n",
        "estadisticas = []\n",
        "\n",
        "for nombre, valores in resultados.items():\n",
        "    estadisticas.append({\n",
        "        'Algoritmo': nombre,\n",
        "        'Mejor': np.min(valores),\n",
        "        'Peor': np.max(valores),\n",
        "        'Media': np.mean(valores),\n",
        "        'Mediana': np.median(valores),\n",
        "        'Desv. Est.': np.std(valores)\n",
        "    })\n",
        "\n",
        "df_estadisticas = pd.DataFrame(estadisticas)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ESTADÍSTICAS DE LOS ALGORITMOS DE OPTIMIZACIÓN\")\n",
        "print(\"=\"*60)\n",
        "print(df_estadisticas.to_string(index=False))\n",
        "\n",
        "# Guardar estadísticas\n",
        "df_estadisticas.to_csv('estadisticas_optimizacion.csv', index=False)\n",
        "print(\"\\nEstadísticas guardadas en 'estadisticas_optimizacion.csv'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Gráfica de convergencia de algoritmos de optimización\n",
        "plt.figure(figsize=(14, 6))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "for nombre, historia in historias.items():\n",
        "    plt.semilogy(historia, label=nombre, linewidth=2)\n",
        "plt.xlabel('Iteración', fontsize=12)\n",
        "plt.ylabel('Valor de la Función Objetivo', fontsize=12)\n",
        "plt.title('Convergencia de Algoritmos de Optimización (escala log)', fontsize=14, fontweight='bold')\n",
        "plt.legend(fontsize=11)\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "datos_boxplot = [resultados[nombre] for nombre in resultados.keys()]\n",
        "plt.boxplot(datos_boxplot, labels=list(resultados.keys()))\n",
        "plt.ylabel('Valor de la Función Objetivo', fontsize=12)\n",
        "plt.title('Distribución de Resultados', fontsize=14, fontweight='bold')\n",
        "plt.xticks(rotation=15, ha='right')\n",
        "plt.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('optimizacion_comparacion.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"Gráficas de optimización generadas y guardadas\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Conclusiones\n",
        "\n",
        "### Métodos Iterativos para Ecuación de Laplace\n",
        "\n",
        "Los resultados muestran que:\n",
        "\n",
        "1. **Gauss-Seidel** es el método más rápido para este problema, convergiendo aproximadamente el doble de rápido que Jacobi\n",
        "2. **Jacobi** es más lento pero más fácil de paralelizar\n",
        "3. **Gradiente Descendente** muestra una convergencia consistente y predecible\n",
        "\n",
        "### Algoritmos de Optimización\n",
        "\n",
        "Los tres algoritmos implementados demuestran diferentes características:\n",
        "\n",
        "1. **Búsqueda Aleatoria**: Simple pero ineficiente, útil para exploración inicial\n",
        "2. **Gradiente Simple**: Convergencia más rápida pero puede quedar atrapado en mínimos locales\n",
        "3. **Método Híbrido**: Combina exploración global con refinamiento local\n",
        "\n",
        "### Archivos Generados\n",
        "\n",
        "- `solucion_laplace.csv`: Solución de la ecuación de Laplace\n",
        "- `convergencia_metodos.csv`: Historia de convergencia de los métodos iterativos\n",
        "- `estadisticas_optimizacion.csv`: Estadísticas de los algoritmos de optimización\n",
        "- `convergencia_metodos.png`: Gráfica de convergencia\n",
        "- `solucion_laplace.png`: Visualización de la solución\n",
        "- `optimizacion_comparacion.png`: Comparación de algoritmos de optimización\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
